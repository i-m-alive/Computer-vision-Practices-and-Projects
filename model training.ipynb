{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c0e136-c66c-4081-9031-7716ff17e38f",
   "metadata": {},
   "source": [
    "# Hand Gesture Model Training\n",
    "**Kumar Shivraj 21BTRCL055**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6733cb2b-30ed-4f4e-ad17-202faff5f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.3202 - loss: 3.3369 - val_accuracy: 0.9000 - val_loss: 0.3531 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 209ms/step - accuracy: 0.7812 - loss: 0.3866 - val_accuracy: 0.9036 - val_loss: 0.3562 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.7152 - loss: 0.7805 - val_accuracy: 0.9679 - val_loss: 0.1424 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 218ms/step - accuracy: 0.7812 - loss: 0.2862 - val_accuracy: 0.9750 - val_loss: 0.1355 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - accuracy: 0.8296 - loss: 0.5081 - val_accuracy: 0.9786 - val_loss: 0.1167 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 208ms/step - accuracy: 0.8438 - loss: 0.1683 - val_accuracy: 0.9643 - val_loss: 0.1127 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8610 - loss: 0.4004 - val_accuracy: 0.9964 - val_loss: 0.0840 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 213ms/step - accuracy: 0.8750 - loss: 0.2407 - val_accuracy: 1.0000 - val_loss: 0.0757 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - accuracy: 0.8901 - loss: 0.3267 - val_accuracy: 0.9857 - val_loss: 0.0728 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 228ms/step - accuracy: 0.9062 - loss: 0.1389 - val_accuracy: 0.9857 - val_loss: 0.0760 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.8858 - loss: 0.3235 - val_accuracy: 1.0000 - val_loss: 0.0439 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 217ms/step - accuracy: 0.9375 - loss: 0.0883 - val_accuracy: 1.0000 - val_loss: 0.0409 - learning_rate: 0.0010\n",
      "Epoch 13/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.9248 - loss: 0.2258 - val_accuracy: 0.9643 - val_loss: 0.0768 - learning_rate: 0.0010\n",
      "Epoch 14/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 219ms/step - accuracy: 0.9375 - loss: 0.1171 - val_accuracy: 0.9679 - val_loss: 0.0798 - learning_rate: 0.0010\n",
      "Epoch 15/15\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.9191 - loss: 0.2594 - val_accuracy: 0.9964 - val_loss: 0.0345 - learning_rate: 0.0010\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.9927 - loss: 0.0399\n",
      "Test Loss: 0.043502286076545715\n",
      "Test Accuracy: 0.9892857074737549\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define image height and width\n",
    "img_height, img_width = 224,224\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Main folder containing labeled subfolders with images\n",
    "main_folder = \"D:\\\\hand img\"\n",
    "\n",
    "# Initialize lists to store image paths and corresponding labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Traverse through each subfolder (class)\n",
    "for label in os.listdir(main_folder):\n",
    "    label_folder = os.path.join(main_folder, label)\n",
    "    if os.path.isdir(label_folder):\n",
    "        # Traverse through images in the current subfolder\n",
    "        for image_name in os.listdir(label_folder):\n",
    "            image_path = os.path.join(label_folder, image_name)\n",
    "            # Append image path and corresponding label to the lists\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(label)\n",
    "# Convert labels to numeric format\n",
    "label_to_index = {label: index for index, label in enumerate(set(labels))}\n",
    "labels = [label_to_index[label] for label in labels]\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "valid_paths, test_paths, valid_labels, test_labels = train_test_split(temp_paths, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0  # Normalize\n",
    "    return img\n",
    "\n",
    "# Load and preprocess training, validation, and testing data\n",
    "train_data = np.array([preprocess_image(image_path) for image_path in train_paths])\n",
    "valid_data = np.array([preprocess_image(image_path) for image_path in valid_paths])\n",
    "test_data = np.array([preprocess_image(image_path) for image_path in test_paths])\n",
    "\n",
    "# Convert labels to categorical format\n",
    "num_classes = len(set(labels))  # Number of unique classes\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "valid_labels = to_categorical(valid_labels, num_classes=num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "# Load pretrained VGG16 model (excluding the top FC layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the pretrained layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# Add custom FC layers on top of the pretrained layers\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Define data augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=None  # No need for preprocess_input as images are already normalized\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow(train_data, train_labels, batch_size=batch_size)\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with data augmentation and callbacks\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd55ac1-acb4-4c62-bef3-d8f5adc32613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"hand_gesture_model3.h5\")\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056d8c1-e45d-425c-8cc4-ec126ef2e250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
